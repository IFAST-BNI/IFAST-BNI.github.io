<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="IFAST: Weakly Supervised Interpretable Face Anti-Spoofing from Single-shot Binocular NIR Images">
  <meta name="keywords" content="Face Anti-Spoofing, Binocular Near-Infrared Images>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>IFAST-BNI</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com"> -->
        <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Contact
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="mailto:jc.huang@siat.ac.cn">
            Jiancheng Huang
          </a>
          <a class="navbar-item" href="mailto:dh.zhou@siat.ac.cn">
            Donghao Zhou
          </a>
          <a class="navbar-item" href="mailto:sf.chen@siat.ac.cn">
            Shifeng Chen
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">IFAST: Weakly Supervised Interpretable<br>Face Anti-Spoofing from Single-Shot<br>Binocular NIR Images</h1>

          <div class="is-size-5 publication-authors"></h1><b>arXiv Preprint (Under Review)</b></div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://huangjch526.github.io/">Jiancheng Huang</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://correr-zhou.github.io/">Donghao Zhou</a><sup>1,2,3*</sup>,</span>
            <span class="author-block">
              <a href="https://people.ucas.ac.cn/~sfchen">Shifeng Chen</a><sup>1‚úù</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,</span>
            <span class="author-block"><sup>2</sup>University of Chinese Academy of Sciences,</span>
            <span class="author-block"><sup>3</sup>The Chinese University of Hong Kong</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://IFAST-BNI.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://IFAST-BNI.github.io"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-database"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://IFAST-BNI.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column is-full-width">
      <img src="static/images/banner.png" style="width: 100%" alt>
  </div>
  </div>
</div> -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Single-shot face anti-spoofing (FAS) is a key technique for securing face recognition systems, and it requires only static images as input. However, single-shot FAS remains a challenging and under-explored problem due to two main reasons: 1) on the data side, learning FAS from RGB images is largely context-dependent, and single-shot images without additional annotations contain limited semantic information. 2) on the model side, existing single-shot FAS models are infeasible to provide proper evidence for their decisions, and FAS methods based on depth estimation require expensive per-pixel annotations. To address these issues, a large binocular NIR image dataset (BNI-FAS) is constructed and published, which contains more than 300,000 real face and plane attack images, and an Interpretable FAS Transformer (IFAST) is proposed that requires only weak supervision to produce interpretable predictions. Our IFAST can produce pixel-wise disparity maps by the proposed disparity estimation Transformer with Dynamic Matching Attention (DMA) block. Besides, a well-designed confidence map generator is adopted to cooperate with the proposed dual-teacher distillation module to obtain the final discriminant results. The comprehensive experiments show that our IFAST can achieve state-of-the-art results on BNI-FAS, proving the effectiveness of the single-shot FAS based on binocular NIR images.
          </p>
      </div>
    </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
        <h2 class="title is-3">New Paradigm of Face Anti-Spoofing</h2>
        <img src="static/images/fas.png" style="width: 70%" alt>
        <div class="content has-text-justified">
        <p>
          <b>Illustration of face anti-spoofing (FAS) from single-shot binocular near-intrared (NIR) Images.</b>
          (a) Multi-shots FAS.
          (b) Single-shot depth-based FAS.
          (c) Our single-shot binocular disparity-based FAS.
          Note that the same colored points are the corresponding maximum attention weight points between the left and right images to emphasize our disparity-based method.
        </p>
        </div>
    </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
        <h2 class="title is-3">Interpretable FAS Transformer</h2>
        <img src="static/images/ifast.png" style="width: 100%" alt>
        <div class="content has-text-justified">
        <p>
          <b>Training pipeline of the proposed Interpretable FAS Transformer (IFAST).</b>
          The input of IFAST is a binocular NIR face image (left and right image). First, an estimated disparity map is obtained by the disparity estimation Transformer. Then, the disparity map and the left image are fed to the confidence map generator. The confidence maps in terms of real face and plane attack are used to complete the classification of FAS. The proposed dual-teacher distillation module is used to support the weakly supervised training.
        </p>
        </div>
    </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
        <h2 class="title is-3">Benchmark Experiments</h2>
        <img src="static/images/benchmark.png" style="width: 100%" alt>
        <div class="content has-text-justified">
        <p>
          <b>Comparison of FAS methods on different test settings.</b>
          Real and Attack denote the setting of positive samples and negative samples in the test set, respectively. The best and second best are in bold and underlined, respectively. The sign "*" denotes that it is a full-supervised method.
        </p>
        </div>
    </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
        <h2 class="title is-3">Visualization Results</h2>
        <img src="static/images/real.jpg" style="width: 100%" alt>
        <div class="content has-text-justified">
        <p>
          <b>Visual comparison of the disparity maps predicted by different methods for estimating the depth of real faces.</b>
          (a) The left image.
          (b) StereoNet.
          (c) PSMNet.
          (d) GwcNet.
          (e) STTR.
          (f) PASMNet.
          (g) Dual-Net.
          (h) BM.
          (i) SGBM.
          (j) IFAST.
          The comparison show that Our IFAST can produce higher visibility.
        </p>
        </div>
    </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    @article{placeholder,
      title  ={placeholder},
      author ={placeholder},
      journal={placeholder},
      year   ={2023}
    }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2212.10066.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Correr-Zhou/RepMode" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="content">
        <p>
          This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. Thanks for thier excellent source code.
        </p>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
